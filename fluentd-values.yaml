image:
  repository: canada/fluentd-with-plugins
  tag: v3.0.2
  pullPolicy: Always

service:
  ports:
    - name: "forward"
      type: ClusterIP
      port: 24224

configMaps:
  useDefaults:
    systemConf: false
    containersInputConf: false
    systemInputConf: false
    forwardInputConf: false
    monitoringConf: false
    outputConf: false

# can be used to add new config or overwrite the default configmaps completely after the configmaps default has been disabled above
extraConfigMaps:
  system.conf: |-
    <source>
      @type forward
      tag applogs
      port 24224
      bind 0.0.0.0
    </source>

    <filter applogs>
      @type parser
      key_name log
      <parse>
        @type multiline_grok
        grok_name_key grok_name
        grok_failure_key grokfailure
        multiline_start_regexp ^[0-9]{4}-[0-9]{2}-[0-9]{2}
        <grok>
            pattern %{SYSLOG5424SD:timestamp2} %{WORD:event}:%{NUMBER:unix_epoch}:%{GREEDYDATA:json_string}
        </grok>
        <grok>
            pattern %{TIMESTAMP_ISO8601:timestamp2}:%{LOGLEVEL:loglevel}:DataAccessLogManager:%{WORD:event}:%{NUMBER:unix_epoch}:%{GREEDYDATA:json_string}
        </grok>
        <grok>
            pattern %{TIMESTAMP_ISO8601:timestamp}:%{LOGLEVEL:loglevel}:%{DATA:role}:%{DATA:event} %{BASE16FLOAT:client_id}:%{USERNAME:user_id} %{GREEDYDATA:pii_lines}
        </grok>
        <grok>
            pattern %{TIMESTAMP_ISO8601:timestamp}:%{LOGLEVEL:loglevel}:%{DATA:role}:%{NUMBER:client_id} %{GREEDYDATA:message} in %{BASE16FLOAT:seconds}
        </grok>
        <grok>
            pattern %{TIMESTAMP_ISO8601:timestamp}:%{LOGLEVEL:loglevel}:%{DATA:role}:%{GREEDYDATA:message}: took %{BASE16FLOAT}
        </grok>
        <grok>
            pattern %{TIMESTAMP_ISO8601:timestamp}:%{LOGLEVEL:loglevel}:%{DATA:logger}:%{GREEDYDATA:event}
        </grok>
        <grok>
            pattern %{GREEDYDATA:message}
        </grok>
        time_key timestamp2
      </parse>
    </filter>
    <match applogs>
      @type rewrite_tag_filter
      <rule>
        key event
        pattern /^DATA_ACCESS_LOGS$/
        tag data_access
      </rule>
      <rule>
        key event
        pattern /^((?!DATA_ACCESS_LOGS).)*$/
        tag services
      </rule>
      <rule>
        key message
        pattern /.*/
        tag services
      </rule>
    </match>

    <match data_access>
      @type elasticsearch
      hosts elasticsearch-client.default.svc:9200
      logstash_format true
      logstash_prefix data-access
      type_name log
    </match>

    <match services>
      @type elasticsearch
      hosts elasticsearch-client.default.svc:9200
      logstash_format true
      logstash_prefix services
      type_name log
    </match>
  kubernetes.conf: |-
    <source>
      @id fluentd-containers.log
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/containers.log.pos
      tag raw.kubernetes.*
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>

    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>

    # Concatenate multi-line logs
    <filter kubernetes.**>
      @id filter_concat
      @type concat
      key message
      multiline_end_regexp /\n$/
      separator ""
      timeout_label @NORMAL
      flush_interval 5
    </filter>

    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      @id filter_kubernetes_metadata
      @type kubernetes_metadata
    </filter>

    # Fixes json fields in Elasticsearch
    <filter kubernetes.**>
      @id filter_parser
      @type parser
      key_name log
      reserve_time true
      reserve_data true
      remove_key_name_field true
      <parse>
        @type multi_format
        <pattern>
          format json
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>

    <match kubernetes.**>
      @type elasticsearch
      hosts elasticsearch-client.default.svc:9200
      logstash_format true
      logstash_prefix kubernetes
      type_name log
    </match>

