---
replicas: 1

# Allows you to add any config files in /usr/share/logstash/config/
# such as logstash.yml and log4j2.properties
logstashConfig:
#  logstash.yml: |
#    key:
#      nestedkey: value
 log4j2.properties: |
    logger.elasticsearchoutput.name = logstash.outputs.elasticsearch
    logger.elasticsearchoutput.level = error

# Allows you to add any pipeline files in /usr/share/logstash/pipeline/
logstashPipeline:
  logstash.conf: |
    input {
      beats {
        port => 5044
        host => "0.0.0.0"
        tags => ["applogs"]
      }
    }
    input {
      beats {
        port => 5045
        host => "0.0.0.0"
        tags => ["k8s"]
      }
    }
    filter {
      if "applogs" in [tags] {
        grok {
            match => { "message" => ["%{SYSLOG5424SD:timestamp2} %{WORD:event}:%{NUMBER:unix_epoch}:%{JSON:json_string}"] }
            add_tag => ["data_access"]
            remove_field => "message"
            patterns_dir => ["./patterns"]
        }
        grok {
            match => { "message" => ["%{TIMESTAMP_ISO8601:timestamp2}:%{LOGLEVEL:loglevel}:DataAccessLogManager:%{WORD:event}:%{NUMBER:unix_epoch}:%{JSON:json_string}"] }
            add_tag => ["data_access"]
            remove_field => "message"
            patterns_dir => ["./patterns"]
        }
        grok {
            match => { "message" => ["%{TIMESTAMP_ISO8601:timestamp}:%{LOGLEVEL:loglevel}:%{DATA:role}:%{DATA:event} %{BASE16FLOAT:client_id}:%{USERNAME:user_id} %{GREEDYDATA:pii_lines}"] }
            add_tag => ["data_access"]
            remove_field => "message"
        }
        grok {
            match => { "message" => ["%{TIMESTAMP_ISO8601:timestamp}:%{LOGLEVEL:loglevel}:%{DATA:role}:%{NUMBER:client_id} %{GREEDYDATA:log} in %{BASE16FLOAT:seconds}"] }
            add_tag => ["services"]
            remove_field => "message"
        }
        grok {
            match => { "message" => ["%{TIMESTAMP_ISO8601:timestamp}:%{LOGLEVEL:loglevel}:%{DATA:role}:%{GREEDYDATA:log}: took %{BASE16FLOAT}"] }
            add_tag => ["services"]
            remove_field => "message"
        }
        grok {
            match => { "message" => ["%{TIMESTAMP_ISO8601:timestamp}:%{LOGLEVEL:loglevel}:%{DATA:logger}:%{GREEDYDATA:event}"] }
            add_tag => ["services"]
            remove_field => "message"
        }
        grok {
            match => { "message" => ["%{GREEDYDATA:log}"] }
            add_tag => ["services"]
            remove_field => "message"
        }
      }
    }
    filter {
      if "data_access" in [tags] {
        mutate { add_field => { "[@metadata][target_index]" => "data_access-%{+yyyy.MM.dd}" } }
      } 
      else if "services" in [tags] {
        mutate { add_field => { "[@metadata][target_index]" => "services-%{+yyyy.MM.dd}" } }
      }
      else {
        mutate { add_field => { "[@metadata][target_index]" => "k8s-%{+yyyy.MM.dd}" } }
      }
    }
    output {
      elasticsearch {
        hosts => "elasticsearch-master.efk:9200"
        manage_template => false
        index => "%{[@metadata][target_index]}"
      }
    }

# Extra environment variables to append to this nodeGroup
# This will be appended to the current 'env:' key. You can use any of the kubernetes env
# syntax here
extraEnvs: []
#  - name: MY_ENVIRONMENT_VAR
#    value: the_value_goes_here

# A list of secrets and their paths to mount inside the pod
secretMounts: []

image: "docker.elastic.co/logstash/logstash-oss"
imageTag: "7.6.2"
imagePullPolicy: "IfNotPresent"
imagePullSecrets: []

podAnnotations:
  co.elastic.logs/enabled: "false"
  fluentbit.io/exclude: "true"

# additionals labels
labels: {}

logstashJavaOpts: "-Xmx1g -Xms1g"

resources:
  requests:
    cpu: "100m"
    memory: "1536Mi"
  limits:
    cpu: "1000m"
    memory: "1536Mi"

volumeClaimTemplate:
  accessModes: [ "ReadWriteOnce" ]
  resources:
    requests:
      storage: 1Gi

rbac:
  create: false
  serviceAccountName: ""

podSecurityPolicy:
  create: false
  name: ""
  spec:
    privileged: true
    fsGroup:
      rule: RunAsAny
    runAsUser:
      rule: RunAsAny
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: RunAsAny
    volumes:
      - secret
      - configMap
      - persistentVolumeClaim

persistence:
  enabled: false
  annotations: {}

extraVolumes: |
  - name: custom-patterns
    configMap:
      name: custom-patterns

extraVolumeMounts: |
  - name: custom-patterns
    mountPath: /usr/share/logstash/patterns/custom-patterns
    subPath: custom-patterns
    readOnly: true

extraContainers: ""
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

extraInitContainers: ""
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

# This is the PriorityClass settings as defined in
# https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
priorityClassName: ""

# By default this will make sure two pods don't end up on the same node
# Changing this to a region would allow you to spread pods across regions
antiAffinityTopologyKey: "kubernetes.io/hostname"

# Hard means that by default pods will only be scheduled if there are enough nodes for them
# and that they will never end up on the same node. Setting this to soft will do this "best effort"
antiAffinity: "hard"

# This is the node affinity settings as defined in
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature
nodeAffinity: {}

# The default is to deploy all pods serially. By setting this to parallel all pods are started at
# the same time when bootstrapping the cluster
podManagementPolicy: "Parallel"

httpPort: 9600

updateStrategy: RollingUpdate

# This is the max unavailable setting for the pod disruption budget
# The default value of 1 will make sure that kubernetes won't allow more than 1
# of your pods to be unavailable during maintenance
maxUnavailable: 1

podSecurityContext:
  fsGroup: 1000
  runAsUser: 1000

securityContext:
  capabilities:
    drop:
    - ALL
  # readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000

# How long to wait for logstash to stop gracefully
terminationGracePeriod: 120

livenessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 300
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1

readinessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 3

## Use an alternate scheduler.
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
schedulerName: ""

nodeSelector: {}
tolerations: []

nameOverride: ""
fullnameOverride: ""

lifecycle: {}
  # preStop:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  # postStart:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

service:
 annotations: {}
 type: ClusterIP
 ports:
   - name: beats
     port: 5044
     protocol: TCP
     targetPort: 5044
   - name: beats-k8s
     port: 5045
     protocol: TCP
     targetPort: 5045
   - name: http
     port: 8080
     protocol: TCP
     targetPort: 8080
